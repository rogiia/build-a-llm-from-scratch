{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d76201f-8955-4bb1-8c8e-9408a5a55398",
   "metadata": {},
   "source": [
    "# Chapter 7. Fine-tuning to follow instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaf65010-8177-4839-a38b-a01a095c5869",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3fbd8d7-25a1-4f39-a89d-da3caf047741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'12.6'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.version.cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f250087a-95da-49c3-bdc6-6139cd8c1f94",
   "metadata": {},
   "source": [
    "## 7.2. Preparing the dataset for supervised instruction fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74216613-3a6e-4ab4-8668-7793f99d7c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries: 1100\n"
     ]
    }
   ],
   "source": [
    "# Listing 7.1. Download the dataset\n",
    "import json\n",
    "import os\n",
    "import urllib\n",
    "\n",
    "def download_and_load_file(file_path, url):\n",
    "    if not os.path.exists(file_path):\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            text_data = response.read().decode(\"utf-8\")\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(text_data)\n",
    "    with open(file_path, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "file_path = \"instruction-data.json\"\n",
    "url = (\n",
    "    \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch\"\n",
    "    \"/main/ch07/01_main-chapter-code/instruction-data.json\"\n",
    ")\n",
    "\n",
    "data = download_and_load_file(file_path, url)\n",
    "print(\"Number of entries:\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88a685f2-bc05-4d55-9eb8-14dca9a5318d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example entry:\n",
      " {'instruction': 'Identify the correct spelling of the following word.', 'input': 'Ocassion', 'output': \"The correct spelling is 'Occasion.'\"}\n"
     ]
    }
   ],
   "source": [
    "print(\"Example entry:\\n\", data[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d4f957b-aaff-48fd-874f-b9c397924417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Another example entry:\n",
      " {'instruction': \"What is an antonym of 'complicated'?\", 'input': '', 'output': \"An antonym of 'complicated' is 'simple'.\"}\n"
     ]
    }
   ],
   "source": [
    "print(\"Another example entry:\\n\", data[999])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5207f47e-6e80-4b19-83ef-4107c48798f4",
   "metadata": {},
   "source": [
    "**Exercise 7.1. Changing prompt styles**\n",
    "\n",
    "After fine-tuning the model with the Alpaca prompt style, try the Phi-3 prompt style shown in figure 7.4 and observe whether it affects the response quality of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "979e2031-d457-432b-9389-8a23ba6b3ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_input(entry):\n",
    "    instruction_text = (\n",
    "        f\"<|user|>\\n{entry['instruction']}\"\n",
    "    )\n",
    "\n",
    "    input_text = f\"\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "\n",
    "    return instruction_text + input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f21c2c86-bd7d-462b-a5e3-ba162b3b9e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 7.2. Implementing the prompt formatting function\n",
    "\n",
    "def format_input(entry):\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. \"\n",
    "        f\"Write a response that appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "\n",
    "    input_text = (\n",
    "        f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "    )\n",
    "    return instruction_text + input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b33104b0-7cf4-4d6c-bfb0-a22817f86bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Identify the correct spelling of the following word.\n",
      "\n",
      "### Input:\n",
      "Ocassion\n",
      "\n",
      "### Response:\n",
      "The correct spelling is 'Occasion.'\n"
     ]
    }
   ],
   "source": [
    "model_input = format_input(data[50])\n",
    "desired_response = f\"\\n\\n### Response:\\n{data[50]['output']}\"\n",
    "print(model_input + desired_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ec901e1-d246-42e5-958a-4500ecd1e2d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What is an antonym of 'complicated'?\n",
      "\n",
      "### Response:\n",
      "An antonym of 'complicated' is 'simple'.\n"
     ]
    }
   ],
   "source": [
    "model_input = format_input(data[999])\n",
    "desired_response = f\"\\n\\n### Response:\\n{data[999]['output']}\"\n",
    "print(model_input + desired_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4672583-77c4-42da-a9d2-8438d39e9f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set length: 935\n",
      "Validation set length: 55\n",
      "Test set length: 110\n"
     ]
    }
   ],
   "source": [
    "# Listing 7.3. Partitioning the dataset\n",
    "train_portion = int(len(data) * 0.85)\n",
    "test_portion = int(len(data) * 0.1)\n",
    "val_portion = len(data) - train_portion - test_portion\n",
    "\n",
    "train_data = data[:train_portion]\n",
    "test_data = data[train_portion:train_portion + test_portion]\n",
    "val_data = data[train_portion + test_portion:]\n",
    "\n",
    "print(\"Training set length:\", len(train_data))\n",
    "print(\"Validation set length:\", len(val_data))\n",
    "print(\"Test set length:\", len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fb6f34-143c-40f3-8862-db4b4be4859a",
   "metadata": {},
   "source": [
    "## 7.3. Organizing data into training batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6716e3b2-0fd0-4054-8f14-224f2c6986ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 7.4. Implementing an instruction dataset class\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class InstructionDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.encoded_texts = []\n",
    "        for entry in data:\n",
    "            instruction_plus_input = format_input(entry)\n",
    "            response_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n",
    "            full_text = instruction_plus_input + response_text\n",
    "            self.encoded_texts.append(\n",
    "                tokenizer.encode(full_text)\n",
    "            )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.encoded_texts[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "edacfdda-d475-4661-93f3-672fb12f1d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50256]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b0aaf9e-73b0-4973-ac4c-bd33fbaca1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_draft_1(\n",
    "    batch,\n",
    "    pad_token_id=50256,\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    batch_max_length = max(len(item)+1 for item in batch)\n",
    "    inputs_1st = []\n",
    "\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        new_item += [pad_token_id]\n",
    "\n",
    "        padded = (\n",
    "            new_item + [pad_token_id] *\n",
    "            (batch_max_length - len(new_item))\n",
    "        )\n",
    "        inputs = torch.tensor(padded[:-1])\n",
    "        inputs_1st.append(inputs)\n",
    "\n",
    "    inputs_tensor = torch.stack(inputs_1st).to(device)\n",
    "    return inputs_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "773a2e6c-7a84-4422-8372-ef3041aa3bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n"
     ]
    }
   ],
   "source": [
    "inputs_1 = [0, 1, 2, 3, 4]\n",
    "inputs_2 = [5, 6]\n",
    "inputs_3 = [7, 8, 9]\n",
    "batch = (\n",
    "    inputs_1,\n",
    "    inputs_2,\n",
    "    inputs_3\n",
    ")\n",
    "print(custom_collate_draft_1(batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f235649-44c1-459b-b5c3-b7dc2b9963ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n",
      "tensor([[    1,     2,     3,     4, 50256],\n",
      "        [    6, 50256, 50256, 50256, 50256],\n",
      "        [    8,     9, 50256, 50256, 50256]])\n"
     ]
    }
   ],
   "source": [
    "def custom_collate_draft_2(\n",
    "    batch,\n",
    "    pad_token_id=50256,\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    batch_max_length = max(len(item)+1 for item in batch)\n",
    "    inputs_1st, targets_1st = [], []\n",
    "\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        new_item += [pad_token_id]\n",
    "\n",
    "        padded = (\n",
    "            new_item + [pad_token_id] *\n",
    "            (batch_max_length - len(new_item))\n",
    "        )\n",
    "        inputs = torch.tensor(padded[:-1])\n",
    "        targets = torch.tensor(padded[1:])\n",
    "        inputs_1st.append(inputs)\n",
    "        targets_1st.append(targets)\n",
    "\n",
    "    inputs_tensor = torch.stack(inputs_1st).to(device)\n",
    "    targets_tensor = torch.stack(targets_1st).to(device)\n",
    "    return inputs_tensor, targets_tensor\n",
    "\n",
    "inputs, targets = custom_collate_draft_2(batch)\n",
    "print(inputs)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6e43540-e41e-40dc-986b-62072b63bb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 7.5. Implementing a custom batch collate function\n",
    "def custom_collate_fn(\n",
    "    batch,\n",
    "    pad_token_id=50256,\n",
    "    ignore_index=-100,\n",
    "    allowed_max_length=None,\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    batch_max_length = max(len(item)+1 for item in batch)\n",
    "    inputs_lst, targets_lst = [], []\n",
    "\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        new_item += [pad_token_id]\n",
    "\n",
    "        padded = (\n",
    "            new_item + [pad_token_id] *\n",
    "            (batch_max_length - len(new_item))\n",
    "        )\n",
    "        inputs = torch.tensor(padded[:-1])\n",
    "        targets = torch.tensor(padded[1:])\n",
    "\n",
    "        mask = targets == pad_token_id\n",
    "        indices = torch.nonzero(mask).squeeze()\n",
    "        if indices.numel() > 1:\n",
    "            targets[indices[1:]] = ignore_index\n",
    "\n",
    "        if allowed_max_length is not None:\n",
    "            inputs = inputs[:allowed_max_length]\n",
    "            targets = targets[:allowed_max_length]\n",
    "\n",
    "        inputs_lst.append(inputs)\n",
    "        targets_lst.append(targets)\n",
    "\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    targets_tensor = torch.stack(targets_lst).to(device)\n",
    "    return inputs_tensor, targets_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f230b272-2980-4c27-bd9e-91f3fbc07f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n",
      "tensor([[    1,     2,     3,     4, 50256],\n",
      "        [    6, 50256,  -100,  -100,  -100],\n",
      "        [    8,     9, 50256,  -100,  -100]])\n"
     ]
    }
   ],
   "source": [
    "inputs, targets = custom_collate_fn(batch)\n",
    "print(inputs)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf4cdc1e-9df0-4b15-a1ec-3b473ec95d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1269)\n"
     ]
    }
   ],
   "source": [
    "logits_1 = torch.tensor(\n",
    "    [[-1.0, 1.0],\n",
    "     [-0.5, 1.5]]\n",
    ")\n",
    "targets_1 = torch.tensor([0, 1])\n",
    "loss_1 = torch.nn.functional.cross_entropy(logits_1, targets_1)\n",
    "print(loss_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29013073-dc8e-464a-a481-bcd06b315e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7936)\n"
     ]
    }
   ],
   "source": [
    "logits_2 = torch.tensor(\n",
    "    [[-1.0, 1.0],\n",
    "     [-0.5, 1.5],\n",
    "     [-0.5, 1.5]]\n",
    ")\n",
    "targets_2 = torch.tensor([0, 1, 1])\n",
    "loss_2 = torch.nn.functional.cross_entropy(logits_2, targets_2)\n",
    "print(loss_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "18ffa284-e146-43e2-b2b7-1c7da64ac85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1269)\n",
      "loss_1 == loss_3: tensor(True)\n"
     ]
    }
   ],
   "source": [
    "targets_3 = torch.tensor([0, 1, -100])\n",
    "loss_3 = torch.nn.functional.cross_entropy(logits_2, targets_3)\n",
    "print(loss_3)\n",
    "print(\"loss_1 == loss_3:\", loss_1 == loss_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f94fc35-ddbe-4d30-b3a4-30d156bdb59c",
   "metadata": {},
   "source": [
    "**Exercise 7.2. Instruction and input masking**\n",
    "\n",
    "After completing the chapter and fine-tuning the model with *InstructionDataset*, replace the instruction and input tokens with the *-100* mask to use the instruction masking method illustrated in figure 7.13. Then evaluate whether this has a positive effect on model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a1b94358-7b71-467e-ac31-a49f987330f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstructionDatasetInputMasking(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "\n",
    "        ##########################################################################################\n",
    "        # New: Separate list for instruction lengths\n",
    "        self.instruction_lengths = []\n",
    "        ##########################################################################################\n",
    "        \n",
    "        self.encoded_texts = []\n",
    "        \n",
    "        for entry in data:\n",
    "            instruction_plus_input = format_input(entry)\n",
    "            response_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n",
    "            full_text = instruction_plus_input + response_text\n",
    "            \n",
    "            self.encoded_texts.append(\n",
    "                tokenizer.encode(full_text)\n",
    "            )\n",
    "\n",
    "            ##########################################################################################\n",
    "            # New: collect instruction lengths\n",
    "            instruction_length = len(tokenizer.encode(instruction_plus_input))\n",
    "            self.instruction_lengths.append(instruction_length)\n",
    "            ##########################################################################################\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        # New: return both instruction lengths and texts separately\n",
    "        return self.instruction_lengths[index], self.encoded_texts[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "043a957f-d33c-4ee3-9d81-de31ed21de09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_input_masking_fn(\n",
    "    batch,\n",
    "    pad_token_id=50256,\n",
    "    ignore_index=-100,\n",
    "    allowed_max_length=None,\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    # Find the longest sequence in the batch\n",
    "    batch_max_length = max(len(item)+1 for instruction_length, item in batch)   # New: batch is now a tuple\n",
    "\n",
    "    # Pad and prepare inputs and targets\n",
    "    inputs_lst, targets_lst = [], []\n",
    "\n",
    "    for instruction_length, item in batch:  # New: batch is now a tuple\n",
    "        new_item = item.copy()\n",
    "        # Add an <|endoftext|> token\n",
    "        new_item += [pad_token_id]\n",
    "        # Pad sequences to max_length\n",
    "        padded = new_item + [pad_token_id] * (batch_max_length - len(new_item))\n",
    "        inputs = torch.tensor(padded[:-1])  # Truncate the last token for inputs\n",
    "        targets = torch.tensor(padded[1:])  # Shift +1 to the right for targets\n",
    "\n",
    "        # Replace all but the first padding tokens in targets by ignore_index\n",
    "        mask = targets == pad_token_id\n",
    "        indices = torch.nonzero(mask).squeeze()\n",
    "        if indices.numel() > 1:\n",
    "            targets[indices[1:]] = ignore_index\n",
    "\n",
    "        ##########################################################################################\n",
    "        # New: Mask all input and instruction tokens in the targets\n",
    "        targets[:instruction_length-1] = -100\n",
    "        ##########################################################################################\n",
    "        \n",
    "        # Optionally truncate to maximum sequence length\n",
    "        if allowed_max_length is not None:\n",
    "            inputs = inputs[:allowed_max_length]\n",
    "            targets = targets[:allowed_max_length]\n",
    "        \n",
    "        inputs_lst.append(inputs)\n",
    "        targets_lst.append(targets)\n",
    "\n",
    "    # Convert list of inputs and targets to tensors and transfer to target device\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    targets_tensor = torch.stack(targets_lst).to(device)\n",
    "\n",
    "    return inputs_tensor, targets_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df33a01b-a211-4021-ba38-a558b8cc9d22",
   "metadata": {},
   "source": [
    "## 7.4. Creating data loaders for an instruction dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2f83e250-82b5-4058-aa66-906387e90557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "afa102f1-fde1-4759-9ae3-147d2e541e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "customized_collate_fn = partial(\n",
    "    custom_collate_fn,\n",
    "    device=device,\n",
    "    allowed_max_length=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "286e6f09-0102-437a-8029-281cf09ae454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 7.6. Initializing the data loaders\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_dataset = InstructionDataset(train_data, tokenizer)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "val_dataset = InstructionDataset(val_data, tokenizer)\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "test_dataset = InstructionDataset(test_data, tokenizer)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d2eb5a-aa54-4fc9-ab3f-07cd683cd3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train loader:\")\n",
    "for inputs, targets in train_loader:\n",
    "    print(inputs.shape, targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37e8f8e-cdb1-4408-916c-d57d8e470f8a",
   "metadata": {},
   "source": [
    "## 7.5. Loading a pretrained LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "346619fa-961d-43ff-84eb-fd5a8b225f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/355M/checkpoint\n",
      "File already exists and is up-to-date: gpt2/355M/encoder.json\n",
      "File already exists and is up-to-date: gpt2/355M/hparams.json\n",
      "File already exists and is up-to-date: gpt2/355M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2/355M/model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2/355M/model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2/355M/vocab.bpe\n"
     ]
    }
   ],
   "source": [
    "# Listing 7.7. Loading the pretrained model\n",
    "from gpt_download import download_and_load_gpt2\n",
    "from chapter4 import GPTModel\n",
    "from chapter5 import load_weights_into_gpt\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024,\n",
    "    \"drop_rate\": 0.0,\n",
    "    \"qkv_bias\": True\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25}\n",
    "}\n",
    "\n",
    "CHOOSE_MODEL = \"gpt2-medium (355M)\"\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "\n",
    "settings, params = download_and_load_gpt2(\n",
    "    model_size=model_size,\n",
    "    models_dir=\"gpt2\"\n",
    ")\n",
    "\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "load_weights_into_gpt(model, params)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5a900da1-346e-44d7-8ab5-27957d42f796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Convert the active sentence to passive: 'The chef cooks the meal every day.'\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "input_text = format_input(val_data[0])\n",
    "print(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "23fd4ae0-19b0-4d93-b8be-667410182141",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but got index is on cpu, different from other tensors on cuda:0 (when checking argument in method wrapper_CUDA__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mchapter5\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m generate, text_to_token_ids, token_ids_to_text\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m token_ids = \u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43midx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_to_token_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m35\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBASE_CONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontext_length\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43meos_id\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50256\u001b[39;49m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m generated_text = token_ids_to_text(token_ids, tokenizer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projectes/build-a-llm-from-scratch/chapter-7/chapter5.py:104\u001b[39m, in \u001b[36mgenerate\u001b[39m\u001b[34m(model, idx, max_new_tokens, context_size, temperature, top_k, eos_id)\u001b[39m\n\u001b[32m    102\u001b[39m idx_cond = idx[:, -context_size:]\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m     logits = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx_cond\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    105\u001b[39m logits = logits[:, -\u001b[32m1\u001b[39m, :]\n\u001b[32m    107\u001b[39m \u001b[38;5;66;03m# New: Filter logits with top_k sampling\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projectes/build-a-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projectes/build-a-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projectes/build-a-llm-from-scratch/chapter-7/chapter4.py:86\u001b[39m, in \u001b[36mGPTModel.forward\u001b[39m\u001b[34m(self, in_idx)\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, in_idx):\n\u001b[32m     85\u001b[39m     batch_size, seq_len = in_idx.shape\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m     tok_embeds = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtok_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     87\u001b[39m     pos_embeds = \u001b[38;5;28mself\u001b[39m.pos_emb(torch.arange(seq_len, device=in_idx.device))\n\u001b[32m     89\u001b[39m     x = tok_embeds + pos_embeds\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projectes/build-a-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projectes/build-a-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projectes/build-a-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192\u001b[39m, in \u001b[36mEmbedding.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projectes/build-a-llm-from-scratch/.venv/lib/python3.12/site-packages/torch/nn/functional.py:2546\u001b[39m, in \u001b[36membedding\u001b[39m\u001b[34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[39m\n\u001b[32m   2540\u001b[39m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[32m   2541\u001b[39m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[32m   2542\u001b[39m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[32m   2543\u001b[39m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[32m   2544\u001b[39m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[32m   2545\u001b[39m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[32m-> \u001b[39m\u001b[32m2546\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Expected all tensors to be on the same device, but got index is on cpu, different from other tensors on cuda:0 (when checking argument in method wrapper_CUDA__index_select)"
     ]
    }
   ],
   "source": [
    "from chapter5 import generate, text_to_token_ids, token_ids_to_text\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(input_text, tokenizer),\n",
    "    max_new_tokens=35,\n",
    "    context_size=BASE_CONFIG[\"context_length\"],\n",
    "    eos_id=50256\n",
    ")\n",
    "generated_text = token_ids_to_text(token_ids, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ac44d5be-f4d4-4b69-ab5c-999b959b2090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Response:\n",
      "\n",
      "The chef cooks the meal every day.\n",
      "\n",
      "### Instruction:\n",
      "\n",
      "Convert the active sentence to passive: 'The chef cooks the\n"
     ]
    }
   ],
   "source": [
    "response_text = generated_text[len(input_text):].strip()\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177da1b1-edae-42d0-96a2-fe8f05185122",
   "metadata": {},
   "source": [
    "## 7.6. Fine-tuning the LLM on Instruction data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fd9e81c6-136e-4073-839a-27290c0ab694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'chapter5' from '/home/roger/Projectes/build-a-llm-from-scratch/chapter-7/chapter5.py'>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import chapter5\n",
    "import importlib\n",
    "importlib.reload(chapter5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b2b925ae-90a9-4fb2-a69f-d9f81bdf0d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chapter5 import (\n",
    "    calc_loss_loader,\n",
    "    train_model_simple\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ee0ab75b-4e90-4212-a5bf-7ca4a04eb979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 3.8258957386016847\n",
      "Validation loss: 3.7619208812713625\n"
     ]
    }
   ],
   "source": [
    "#device = \"cuda\"\n",
    "model.to(device)\n",
    "torch.manual_seed(123)\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(\n",
    "        train_loader, model, device, num_batches=5\n",
    "    )\n",
    "    val_loss = calc_loss_loader(\n",
    "        val_loader, model, device, num_batches=5\n",
    "    )\n",
    "\n",
    "    print(\"Training loss:\", train_loss)\n",
    "    print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f0821961-60d4-44e4-8fd0-f8f51c169f26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aff5b745-dc1d-4419-9364-d4371c28afb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 2.637, Val loss 2.626\n",
      "Ep 1 (Step 000005): Train loss 1.174, Val loss 1.102\n",
      "Ep 1 (Step 000010): Train loss 0.872, Val loss 0.945\n",
      "Ep 1 (Step 000015): Train loss 0.856, Val loss 0.906\n",
      "Ep 1 (Step 000020): Train loss 0.776, Val loss 0.881\n",
      "Ep 1 (Step 000025): Train loss 0.753, Val loss 0.859\n",
      "Ep 1 (Step 000030): Train loss 0.798, Val loss 0.836\n",
      "Ep 1 (Step 000035): Train loss 0.714, Val loss 0.808\n",
      "Ep 1 (Step 000040): Train loss 0.672, Val loss 0.806\n",
      "Ep 1 (Step 000045): Train loss 0.633, Val loss 0.790\n",
      "Ep 1 (Step 000050): Train loss 0.662, Val loss 0.783\n",
      "Ep 1 (Step 000055): Train loss 0.760, Val loss 0.764\n",
      "Ep 1 (Step 000060): Train loss 0.719, Val loss 0.743\n",
      "Ep 1 (Step 000065): Train loss 0.652, Val loss 0.735\n",
      "Ep 1 (Step 000070): Train loss 0.532, Val loss 0.729\n",
      "Ep 1 (Step 000075): Train loss 0.569, Val loss 0.729\n",
      "Ep 1 (Step 000080): Train loss 0.605, Val loss 0.725\n",
      "Ep 1 (Step 000085): Train loss 0.509, Val loss 0.709\n",
      "Ep 1 (Step 000090): Train loss 0.562, Val loss 0.691\n",
      "Ep 1 (Step 000095): Train loss 0.500, Val loss 0.681\n",
      "Ep 1 (Step 000100): Train loss 0.502, Val loss 0.677\n",
      "Ep 1 (Step 000105): Train loss 0.564, Val loss 0.670\n",
      "Ep 1 (Step 000110): Train loss 0.555, Val loss 0.667\n",
      "Ep 1 (Step 000115): Train loss 0.508, Val loss 0.664\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The meal is prepared every day by the chef.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive:\n",
      "Ep 2 (Step 000120): Train loss 0.435, Val loss 0.672\n",
      "Ep 2 (Step 000125): Train loss 0.450, Val loss 0.687\n",
      "Ep 2 (Step 000130): Train loss 0.447, Val loss 0.682\n",
      "Ep 2 (Step 000135): Train loss 0.405, Val loss 0.681\n",
      "Ep 2 (Step 000140): Train loss 0.409, Val loss 0.681\n",
      "Ep 2 (Step 000145): Train loss 0.368, Val loss 0.681\n",
      "Ep 2 (Step 000150): Train loss 0.381, Val loss 0.676\n",
      "Ep 2 (Step 000155): Train loss 0.412, Val loss 0.676\n",
      "Ep 2 (Step 000160): Train loss 0.415, Val loss 0.684\n",
      "Ep 2 (Step 000165): Train loss 0.379, Val loss 0.686\n",
      "Ep 2 (Step 000170): Train loss 0.323, Val loss 0.683\n",
      "Ep 2 (Step 000175): Train loss 0.337, Val loss 0.671\n",
      "Ep 2 (Step 000180): Train loss 0.392, Val loss 0.658\n",
      "Ep 2 (Step 000185): Train loss 0.415, Val loss 0.660\n",
      "Ep 2 (Step 000190): Train loss 0.340, Val loss 0.651\n",
      "Ep 2 (Step 000195): Train loss 0.330, Val loss 0.638\n",
      "Ep 2 (Step 000200): Train loss 0.310, Val loss 0.638\n",
      "Ep 2 (Step 000205): Train loss 0.351, Val loss 0.634\n",
      "Ep 2 (Step 000210): Train loss 0.366, Val loss 0.632\n",
      "Ep 2 (Step 000215): Train loss 0.396, Val loss 0.636\n",
      "Ep 2 (Step 000220): Train loss 0.300, Val loss 0.648\n",
      "Ep 2 (Step 000225): Train loss 0.348, Val loss 0.660\n",
      "Ep 2 (Step 000230): Train loss 0.294, Val loss 0.657\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The meal is cooked every day by the chef.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: What is the capital of the United Kingdom\n",
      "Training completed in 34.67 minutes.\n"
     ]
    }
   ],
   "source": [
    "# Listing 7.8. Instruction fine-tuning the pretrained LLM\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "torch.manual_seed(123)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), lr=5e-5, weight_decay=0.1\n",
    ")\n",
    "num_epochs = 2\n",
    "\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device=device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=format_input(val_data[0]), tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "98bc2069-2956-4063-a01e-ecde03936d02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from chapter5 import plot_losses\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c289a7a-f041-43df-a63c-9d6b649e8f32",
   "metadata": {},
   "source": [
    "**Exercise 7.3. Fine-tuning on the original Alpaca dataset**\n",
    "\n",
    "The Alpaca dataset, by researchers at Stanford, is one of the earliest and most popular openly shared instruction datasets, consisting of 52,002 entries. As an alternative to the *instruction-data.json* file we use here, consider fine-tuning an LLM on this dataset. The dataset is available at https://mng.bz/NBnE.\n",
    "\n",
    "This dataset contains 52,002 entries, which is approximately 50 times more than those we used here, and most entries are longer. Thus, I highly recommend using a GPU to conduct the training, which will accelerate the fine-tuning process. If you encounter out-of-memory errors, consider reducing the *batch_size* from 8 to 4, 2, or even 1. Lowering the *allowed_max_length* from 1,024 to 512 or 256 can also help manage memory problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f9c4446-10fe-48b1-b3c4-4b9508e0075b",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_file_path = \"alpaca_data.json\"\n",
    "alpaca_url = \"https://mng.bz/NBnE\"\n",
    "alpaca_data = download_and_load_file(alpaca_file_path, alpaca_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b832ed6-e24d-43f8-b5f4-b84ae4c58ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set length: 44201\n",
      "Validation set length: 2601\n",
      "Test set length: 5200\n"
     ]
    }
   ],
   "source": [
    "train_portion = int(len(alpaca_data) * 0.85)\n",
    "test_portion = int(len(alpaca_data) * 0.1)\n",
    "val_portion = len(alpaca_data) - train_portion - test_portion\n",
    "\n",
    "train_data = alpaca_data[:train_portion]\n",
    "test_data = alpaca_data[train_portion:train_portion + test_portion]\n",
    "val_data = alpaca_data[train_portion + test_portion:]\n",
    "\n",
    "print(\"Training set length:\", len(train_data))\n",
    "print(\"Validation set length:\", len(val_data))\n",
    "print(\"Test set length:\", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca8a12a5-8af2-4fe2-b46a-4d1a9102dc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 0\n",
    "batch_size = 1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "customized_collate_fn = partial(\n",
    "    custom_collate_fn,\n",
    "    device=device,\n",
    "    allowed_max_length=256\n",
    ")\n",
    "\n",
    "alpaca_train_dataset = InstructionDataset(train_data, tokenizer)\n",
    "alpaca_train_loader = DataLoader(\n",
    "    alpaca_train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "alpaca_val_dataset = InstructionDataset(val_data, tokenizer)\n",
    "alpaca_val_loader = DataLoader(\n",
    "    alpaca_val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "alpaca_test_dataset = InstructionDataset(test_data, tokenizer)\n",
    "alpaca_test_loader = DataLoader(\n",
    "    alpaca_test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7335d521-3535-4f6c-a92f-aca31f9281e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-01 18:54:23.473217: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/124M/checkpoint\n",
      "File already exists and is up-to-date: gpt2/124M/encoder.json\n",
      "File already exists and is up-to-date: gpt2/124M/hparams.json\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2/124M/vocab.bpe\n"
     ]
    }
   ],
   "source": [
    "from gpt_download import download_and_load_gpt2\n",
    "from chapter4 import GPTModel\n",
    "from chapter5 import load_weights_into_gpt\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024,\n",
    "    \"drop_rate\": 0.0,\n",
    "    \"qkv_bias\": True\n",
    "}\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25}\n",
    "}\n",
    "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "\n",
    "settings, params = download_and_load_gpt2(\n",
    "    model_size=model_size,\n",
    "    models_dir=\"gpt2\"\n",
    ")\n",
    "\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "load_weights_into_gpt(model, params)\n",
    "model.eval()\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "19a893e1-14c0-41b0-a38b-b63a73a26b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 3.250, Val loss 3.279\n",
      "Ep 1 (Step 000005): Train loss 1.987, Val loss 1.818\n",
      "Ep 1 (Step 000010): Train loss 1.019, Val loss 1.372\n",
      "Ep 1 (Step 000015): Train loss 1.320, Val loss 1.285\n",
      "Ep 1 (Step 000020): Train loss 0.794, Val loss 1.220\n",
      "Ep 1 (Step 000025): Train loss 0.878, Val loss 1.163\n",
      "Ep 1 (Step 000030): Train loss 0.978, Val loss 1.136\n",
      "Ep 1 (Step 000035): Train loss 1.111, Val loss 1.125\n",
      "Ep 1 (Step 000040): Train loss 1.157, Val loss 1.076\n",
      "Ep 1 (Step 000045): Train loss 0.751, Val loss 1.048\n",
      "Ep 1 (Step 000050): Train loss 1.157, Val loss 1.020\n",
      "Ep 1 (Step 000055): Train loss 1.190, Val loss 1.013\n",
      "Ep 1 (Step 000060): Train loss 1.122, Val loss 1.035\n",
      "Ep 1 (Step 000065): Train loss 0.934, Val loss 1.035\n",
      "Ep 1 (Step 000070): Train loss 0.625, Val loss 1.044\n",
      "Ep 1 (Step 000075): Train loss 0.685, Val loss 1.042\n",
      "Ep 1 (Step 000080): Train loss 0.910, Val loss 1.053\n",
      "Ep 1 (Step 000085): Train loss 0.699, Val loss 1.076\n",
      "Ep 1 (Step 000090): Train loss 0.859, Val loss 1.090\n",
      "Ep 1 (Step 000095): Train loss 0.782, Val loss 1.089\n",
      "Ep 1 (Step 000100): Train loss 0.734, Val loss 1.057\n",
      "Ep 1 (Step 000105): Train loss 0.750, Val loss 1.043\n",
      "Ep 1 (Step 000110): Train loss 1.076, Val loss 1.037\n",
      "Ep 1 (Step 000115): Train loss 0.771, Val loss 1.038\n",
      "Ep 1 (Step 000120): Train loss 1.275, Val loss 1.047\n",
      "Ep 1 (Step 000125): Train loss 0.712, Val loss 1.047\n",
      "Ep 1 (Step 000130): Train loss 0.838, Val loss 1.062\n",
      "Ep 1 (Step 000135): Train loss 0.738, Val loss 1.078\n",
      "Ep 1 (Step 000140): Train loss 0.944, Val loss 1.076\n",
      "Ep 1 (Step 000145): Train loss 0.862, Val loss 1.060\n",
      "Ep 1 (Step 000150): Train loss 0.673, Val loss 1.054\n",
      "Ep 1 (Step 000155): Train loss 0.822, Val loss 1.051\n",
      "Ep 1 (Step 000160): Train loss 0.690, Val loss 1.055\n",
      "Ep 1 (Step 000165): Train loss 1.174, Val loss 1.050\n",
      "Ep 1 (Step 000170): Train loss 0.748, Val loss 1.029\n",
      "Ep 1 (Step 000175): Train loss 1.113, Val loss 1.025\n",
      "Ep 1 (Step 000180): Train loss 0.674, Val loss 1.024\n",
      "Ep 1 (Step 000185): Train loss 0.835, Val loss 1.020\n",
      "Ep 1 (Step 000190): Train loss 0.967, Val loss 1.028\n",
      "Ep 1 (Step 000195): Train loss 0.765, Val loss 1.033\n",
      "Ep 1 (Step 000200): Train loss 1.099, Val loss 1.027\n",
      "Ep 1 (Step 000205): Train loss 1.069, Val loss 1.036\n",
      "Ep 1 (Step 000210): Train loss 1.122, Val loss 1.035\n",
      "Ep 1 (Step 000215): Train loss 0.563, Val loss 1.022\n",
      "Ep 1 (Step 000220): Train loss 0.528, Val loss 1.008\n",
      "Ep 1 (Step 000225): Train loss 0.475, Val loss 0.973\n",
      "Ep 1 (Step 000230): Train loss 0.766, Val loss 0.958\n",
      "Ep 1 (Step 000235): Train loss 0.925, Val loss 0.949\n",
      "Ep 1 (Step 000240): Train loss 0.887, Val loss 0.943\n",
      "Ep 1 (Step 000245): Train loss 0.895, Val loss 0.943\n",
      "Ep 1 (Step 000250): Train loss 1.012, Val loss 0.938\n",
      "Ep 1 (Step 000255): Train loss 0.521, Val loss 0.937\n",
      "Ep 1 (Step 000260): Train loss 0.898, Val loss 0.937\n",
      "Ep 1 (Step 000265): Train loss 0.532, Val loss 0.943\n",
      "Ep 1 (Step 000270): Train loss 0.757, Val loss 0.950\n",
      "Ep 1 (Step 000275): Train loss 0.862, Val loss 0.959\n",
      "Ep 1 (Step 000280): Train loss 0.744, Val loss 0.972\n",
      "Ep 1 (Step 000285): Train loss 0.830, Val loss 0.976\n",
      "Ep 1 (Step 000290): Train loss 0.759, Val loss 0.993\n",
      "Ep 1 (Step 000295): Train loss 0.872, Val loss 1.000\n",
      "Ep 1 (Step 000300): Train loss 0.922, Val loss 0.998\n",
      "Ep 1 (Step 000305): Train loss 0.783, Val loss 0.993\n",
      "Ep 1 (Step 000310): Train loss 0.679, Val loss 0.981\n",
      "Ep 1 (Step 000315): Train loss 0.644, Val loss 0.976\n",
      "Ep 1 (Step 000320): Train loss 0.919, Val loss 0.970\n",
      "Ep 1 (Step 000325): Train loss 0.697, Val loss 0.969\n",
      "Ep 1 (Step 000330): Train loss 0.574, Val loss 0.972\n",
      "Ep 1 (Step 000335): Train loss 0.659, Val loss 0.970\n",
      "Ep 1 (Step 000340): Train loss 1.282, Val loss 0.972\n",
      "Ep 1 (Step 000345): Train loss 0.901, Val loss 0.981\n",
      "Ep 1 (Step 000350): Train loss 0.615, Val loss 1.001\n",
      "Ep 1 (Step 000355): Train loss 0.574, Val loss 0.989\n",
      "Ep 1 (Step 000360): Train loss 0.914, Val loss 0.972\n",
      "Ep 1 (Step 000365): Train loss 0.564, Val loss 0.960\n",
      "Ep 1 (Step 000370): Train loss 0.846, Val loss 0.955\n",
      "Ep 1 (Step 000375): Train loss 0.682, Val loss 0.939\n",
      "Ep 1 (Step 000380): Train loss 0.602, Val loss 0.923\n",
      "Ep 1 (Step 000385): Train loss 0.950, Val loss 0.937\n",
      "Ep 1 (Step 000390): Train loss 0.519, Val loss 0.942\n",
      "Ep 1 (Step 000395): Train loss 0.849, Val loss 0.943\n",
      "Ep 1 (Step 000400): Train loss 0.653, Val loss 0.936\n",
      "Ep 1 (Step 000405): Train loss 0.705, Val loss 0.921\n",
      "Ep 1 (Step 000410): Train loss 0.678, Val loss 0.920\n",
      "Ep 1 (Step 000415): Train loss 1.157, Val loss 0.926\n",
      "Ep 1 (Step 000420): Train loss 0.847, Val loss 0.909\n",
      "Ep 1 (Step 000425): Train loss 0.596, Val loss 0.903\n",
      "Ep 1 (Step 000430): Train loss 0.646, Val loss 0.917\n",
      "Ep 1 (Step 000435): Train loss 0.545, Val loss 0.920\n",
      "Ep 1 (Step 000440): Train loss 0.540, Val loss 0.886\n",
      "Ep 1 (Step 000445): Train loss 0.506, Val loss 0.873\n",
      "Ep 1 (Step 000450): Train loss 0.477, Val loss 0.874\n",
      "Ep 1 (Step 000455): Train loss 0.774, Val loss 0.878\n",
      "Ep 1 (Step 000460): Train loss 0.676, Val loss 0.885\n",
      "Ep 1 (Step 000465): Train loss 0.630, Val loss 0.873\n",
      "Ep 1 (Step 000470): Train loss 0.434, Val loss 0.867\n",
      "Ep 1 (Step 000475): Train loss 0.622, Val loss 0.868\n",
      "Ep 1 (Step 000480): Train loss 0.855, Val loss 0.872\n",
      "Ep 1 (Step 000485): Train loss 0.513, Val loss 0.884\n",
      "Ep 1 (Step 000490): Train loss 0.621, Val loss 0.895\n",
      "Ep 1 (Step 000495): Train loss 0.522, Val loss 0.898\n",
      "Ep 1 (Step 000500): Train loss 0.634, Val loss 0.901\n",
      "Ep 1 (Step 000505): Train loss 0.999, Val loss 0.904\n",
      "Ep 1 (Step 000510): Train loss 0.620, Val loss 0.901\n",
      "Ep 1 (Step 000515): Train loss 0.502, Val loss 0.900\n",
      "Ep 1 (Step 000520): Train loss 0.662, Val loss 0.907\n",
      "Ep 1 (Step 000525): Train loss 0.722, Val loss 0.898\n",
      "Ep 1 (Step 000530): Train loss 0.640, Val loss 0.880\n",
      "Ep 1 (Step 000535): Train loss 0.674, Val loss 0.876\n",
      "Ep 1 (Step 000540): Train loss 0.654, Val loss 0.867\n",
      "Ep 1 (Step 000545): Train loss 0.676, Val loss 0.874\n",
      "Ep 1 (Step 000550): Train loss 0.721, Val loss 0.874\n",
      "Ep 1 (Step 000555): Train loss 0.634, Val loss 0.881\n",
      "Ep 1 (Step 000560): Train loss 0.704, Val loss 0.877\n",
      "Ep 1 (Step 000565): Train loss 0.627, Val loss 0.945\n",
      "Ep 1 (Step 000570): Train loss 0.544, Val loss 0.890\n",
      "Ep 1 (Step 000575): Train loss 0.541, Val loss 0.883\n",
      "Ep 1 (Step 000580): Train loss 0.780, Val loss 0.881\n",
      "Ep 1 (Step 000585): Train loss 0.899, Val loss 0.899\n",
      "Ep 1 (Step 000590): Train loss 0.535, Val loss 0.899\n",
      "Ep 1 (Step 000595): Train loss 0.731, Val loss 0.889\n",
      "Ep 1 (Step 000600): Train loss 0.516, Val loss 0.904\n",
      "Ep 1 (Step 000605): Train loss 0.494, Val loss 0.914\n",
      "Ep 1 (Step 000610): Train loss 0.744, Val loss 0.911\n",
      "Ep 1 (Step 000615): Train loss 0.607, Val loss 0.907\n",
      "Ep 1 (Step 000620): Train loss 0.828, Val loss 0.886\n",
      "Ep 1 (Step 000625): Train loss 0.517, Val loss 0.885\n",
      "Ep 1 (Step 000630): Train loss 0.970, Val loss 0.894\n",
      "Ep 1 (Step 000635): Train loss 0.732, Val loss 0.903\n",
      "Ep 1 (Step 000640): Train loss 0.534, Val loss 0.881\n",
      "Ep 1 (Step 000645): Train loss 0.536, Val loss 0.884\n",
      "Ep 1 (Step 000650): Train loss 0.682, Val loss 0.874\n",
      "Ep 1 (Step 000655): Train loss 0.460, Val loss 0.879\n",
      "Ep 1 (Step 000660): Train loss 0.563, Val loss 0.887\n",
      "Ep 1 (Step 000665): Train loss 0.683, Val loss 0.891\n",
      "Ep 1 (Step 000670): Train loss 0.634, Val loss 0.888\n",
      "Ep 1 (Step 000675): Train loss 0.420, Val loss 0.889\n",
      "Ep 1 (Step 000680): Train loss 0.577, Val loss 0.885\n",
      "Ep 1 (Step 000685): Train loss 0.355, Val loss 0.882\n",
      "Ep 1 (Step 000690): Train loss 0.657, Val loss 0.886\n",
      "Ep 1 (Step 000695): Train loss 0.410, Val loss 0.827\n",
      "Ep 1 (Step 000700): Train loss 0.528, Val loss 0.799\n",
      "Ep 1 (Step 000705): Train loss 0.539, Val loss 0.775\n",
      "Ep 1 (Step 000710): Train loss 0.647, Val loss 0.753\n",
      "Ep 1 (Step 000715): Train loss 0.512, Val loss 0.750\n",
      "Ep 1 (Step 000720): Train loss 0.340, Val loss 0.758\n",
      "Ep 1 (Step 000725): Train loss 0.608, Val loss 0.769\n",
      "Ep 1 (Step 000730): Train loss 0.649, Val loss 0.780\n",
      "Ep 1 (Step 000735): Train loss 0.510, Val loss 0.779\n",
      "Ep 1 (Step 000740): Train loss 0.490, Val loss 0.780\n",
      "Ep 1 (Step 000745): Train loss 0.499, Val loss 0.780\n",
      "Ep 1 (Step 000750): Train loss 0.511, Val loss 0.774\n",
      "Ep 1 (Step 000755): Train loss 0.670, Val loss 0.779\n",
      "Ep 1 (Step 000760): Train loss 0.813, Val loss 0.787\n",
      "Ep 1 (Step 000765): Train loss 0.422, Val loss 0.799\n",
      "Ep 1 (Step 000770): Train loss 0.635, Val loss 0.799\n",
      "Ep 1 (Step 000775): Train loss 0.530, Val loss 0.783\n",
      "Ep 1 (Step 000780): Train loss 0.583, Val loss 0.773\n",
      "Ep 1 (Step 000785): Train loss 0.421, Val loss 0.764\n",
      "Ep 1 (Step 000790): Train loss 0.667, Val loss 0.768\n",
      "Ep 1 (Step 000795): Train loss 0.528, Val loss 0.774\n",
      "Ep 1 (Step 000800): Train loss 0.649, Val loss 0.777\n",
      "Ep 1 (Step 000805): Train loss 0.667, Val loss 0.775\n",
      "Ep 1 (Step 000810): Train loss 0.613, Val loss 0.769\n",
      "Ep 1 (Step 000815): Train loss 0.448, Val loss 0.761\n",
      "Ep 1 (Step 000820): Train loss 0.803, Val loss 0.756\n",
      "Ep 1 (Step 000825): Train loss 0.562, Val loss 0.760\n",
      "Ep 1 (Step 000830): Train loss 0.520, Val loss 0.765\n",
      "Ep 1 (Step 000835): Train loss 0.567, Val loss 0.768\n",
      "Ep 1 (Step 000840): Train loss 0.480, Val loss 0.768\n",
      "Ep 1 (Step 000845): Train loss 0.431, Val loss 0.765\n",
      "Ep 1 (Step 000850): Train loss 0.521, Val loss 0.766\n",
      "Ep 1 (Step 000855): Train loss 0.464, Val loss 0.764\n",
      "Ep 1 (Step 000860): Train loss 0.665, Val loss 0.762\n",
      "Ep 1 (Step 000865): Train loss 0.477, Val loss 0.764\n",
      "Ep 1 (Step 000870): Train loss 0.459, Val loss 0.771\n",
      "Ep 1 (Step 000875): Train loss 0.581, Val loss 0.731\n",
      "Ep 1 (Step 000880): Train loss 0.374, Val loss 0.718\n",
      "Ep 1 (Step 000885): Train loss 0.488, Val loss 0.701\n",
      "Ep 1 (Step 000890): Train loss 0.668, Val loss 0.694\n",
      "Ep 1 (Step 000895): Train loss 0.433, Val loss 0.691\n",
      "Ep 1 (Step 000900): Train loss 0.436, Val loss 0.691\n",
      "Ep 1 (Step 000905): Train loss 0.430, Val loss 0.687\n",
      "Ep 1 (Step 000910): Train loss 0.549, Val loss 0.686\n",
      "Ep 1 (Step 000915): Train loss 0.516, Val loss 0.681\n",
      "Ep 1 (Step 000920): Train loss 0.502, Val loss 0.676\n",
      "Ep 1 (Step 000925): Train loss 0.437, Val loss 0.673\n",
      "Ep 1 (Step 000930): Train loss 0.549, Val loss 0.670\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The meal every day is cooked by the chef.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.  ### Input: The chef prepares the meal every day.\n",
      "Training completed in 4.95 minutes.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "torch.manual_seed(123)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), lr=5e-5, weight_decay=0.1\n",
    ")\n",
    "num_epochs = 1\n",
    "\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, alpaca_train_loader, alpaca_val_loader, optimizer, device=device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=format_input(val_data[0]), tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c973705f-12b9-440a-80a6-e7c5ca23c189",
   "metadata": {},
   "source": [
    "## 7.7. Extracting and saving responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "59fc9810-7114-48e9-ac8b-d67e8a81e254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Rewrite the sentence using a simile.\n",
      "\n",
      "### Input:\n",
      "The car is very fast.\n",
      "\n",
      "Correct response:\n",
      ">> The car is as fast as lightning.\n",
      "\n",
      "Model response:\n",
      ">> The car is as fast as a bullet.\n",
      "-----------------------------\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What type of cloud is typically associated with thunderstorms?\n",
      "\n",
      "Correct response:\n",
      ">> The type of cloud typically associated with thunderstorms is cumulonimbus.\n",
      "\n",
      "Model response:\n",
      ">> The type of cloud typically associated with thunderstorms is a tropical storm.\n",
      "-----------------------------\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Name the author of 'Pride and Prejudice'.\n",
      "\n",
      "Correct response:\n",
      ">> Jane Austen.\n",
      "\n",
      "Model response:\n",
      ">> The author of 'Pride and Prejudice' is Robert Frost.\n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "for entry in test_data[:3]:\n",
    "    input_text = format_input(entry)\n",
    "    token_ids = generate(\n",
    "        model=model,\n",
    "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
    "        max_new_tokens=256,\n",
    "        context_size=BASE_CONFIG[\"context_length\"],\n",
    "        eos_id=50256\n",
    "    )\n",
    "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "\n",
    "    response_text = (\n",
    "        generated_text[len(input_text):]\n",
    "        .replace(\"### Response:\", \"\")\n",
    "        .strip()\n",
    "    )\n",
    "    print(input_text)\n",
    "    print(f\"\\nCorrect response:\\n>> {entry['output']}\")\n",
    "    print(f\"\\nModel response:\\n>> {response_text.strip()}\")\n",
    "    print(\"-----------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e4c27302-46fe-42a0-a981-8962f92a45d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 110/110 [03:33<00:00,  1.94s/it]\n"
     ]
    }
   ],
   "source": [
    "# Listing 7.9. Generating test set responses\n",
    "from tqdm import tqdm\n",
    "\n",
    "for i, entry in tqdm(enumerate(test_data), total=len(test_data)):\n",
    "    input_text = format_input(entry)\n",
    "\n",
    "    token_ids = generate(\n",
    "        model=model,\n",
    "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
    "        max_new_tokens=256,\n",
    "        context_size=BASE_CONFIG[\"context_length\"],\n",
    "        eos_id=50256\n",
    "    )\n",
    "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "\n",
    "    response_text = (\n",
    "        generated_text[len(input_text):]\n",
    "        .replace(\"### Response:\", \"\")\n",
    "        .strip()\n",
    "    )\n",
    "    test_data[i][\"model_response\"] = response_text\n",
    "\n",
    "with open(\"instruction-data-with-response.json\", \"w\") as file:\n",
    "    json.dump(test_data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "68bb2660-1283-4483-8c5a-ba9ee2ddf259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': 'Rewrite the sentence using a simile.', 'input': 'The car is very fast.', 'output': 'The car is as fast as lightning.', 'model_response': 'The car is as fast as a bullet.'}\n"
     ]
    }
   ],
   "source": [
    "print(test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cf23ddb8-e1a3-4369-a9a5-e5c53e436070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as gpt2-small124M-sft.pth\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "file_name = f\"{re.sub(r'[ ()]', '', CHOOSE_MODEL) }-sft.pth\"\n",
    "torch.save(model.state_dict(), file_name)\n",
    "print(f\"Model saved as {file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc76d01-beac-4ae8-bf8f-585b3e287792",
   "metadata": {},
   "source": [
    "## 7.8. Evaluating the fine-tuned LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f6fbdb3b-17ba-4988-a852-c7b22560924f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama running: True\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "def check_if_running(process_name):\n",
    "    running = False\n",
    "    for proc in psutil.process_iter([\"name\"]):\n",
    "        if process_name in proc.info[\"name\"]:\n",
    "            running = True\n",
    "            break\n",
    "    return running\n",
    "\n",
    "ollama_running = check_if_running(\"ollama\")\n",
    "\n",
    "if not ollama_running:\n",
    "    raise RuntimeError(\n",
    "        \"Ollama not running. Launch ollama before proceeding.\"\n",
    "    )\n",
    "print(\"Ollama running:\", check_if_running(\"ollama\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ab011d00-7f46-4c5e-a051-b44a36e7ec17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "def query_model(\n",
    "    prompt,\n",
    "    model=\"gemma3\",\n",
    "    url=\"http://localhost:11434/api/chat\"\n",
    "):\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"options\": {\n",
    "            \"seed\": 123,\n",
    "            \"temperature\": 0,\n",
    "            \"num_ctx\": 2048\n",
    "        }\n",
    "    }\n",
    "\n",
    "    payload = json.dumps(data).encode(\"utf-8\")\n",
    "    request = urllib.request.Request(\n",
    "        url,\n",
    "        data=payload,\n",
    "        method=\"POST\"\n",
    "    )\n",
    "\n",
    "    request.add_header(\"Content-Type\", \"application/json\")\n",
    "\n",
    "    response_data = \"\"\n",
    "    with urllib.request.urlopen(request) as response:\n",
    "        while True:\n",
    "            line = response.readline().decode(\"utf-8\")\n",
    "            if not line:\n",
    "                break\n",
    "            response_json = json.loads(line)\n",
    "            response_data += response_json[\"message\"][\"content\"]\n",
    "    return response_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3c96b1e7-57e7-4950-a971-71755ee9c8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llamas are surprisingly adaptable eaters! Here's a breakdown of what they typically eat:\n",
      "\n",
      "**1. Primarily Grasses:** This is the foundation of their diet. They'll happily munch on a wide variety of grasses, including:\n",
      "\n",
      "*   **Timothy Hay:** This is considered a staple for llamas and is a good source of fiber.\n",
      "*   **Orchard Grass:** Another popular choice, rich in nutrients.\n",
      "*   **Brome Grass:** A common and nutritious option.\n",
      "*   **Fescue:** Can be fed in moderation, as some types can cause health issues.\n",
      "*   **Various Wild Grasses:** Llamas are often found grazing on native grasses in their natural habitats.\n",
      "\n",
      "\n",
      "**2. Browse (Leaves, Twigs, and Branches):** Llamas are natural browsers and will supplement their grass diet with:\n",
      "\n",
      "*   **Tree Branches:** They'll happily nibble on branches from shrubs and trees.\n",
      "*   **Leaves:**  They'll eat leaves from bushes and trees.\n",
      "*   **Twigs:**  They enjoy chewing on twigs.\n",
      "\n",
      "**3. Supplements (Especially Important for Pet Llamas):**\n",
      "\n",
      "*   **Commercial Llama Feed:**  These are formulated to meet their specific nutritional needs.\n",
      "*   **Minerals:** Llamas need access to minerals, especially salt.  A loose salt block is essential.\n",
      "*   **Vitamin Supplements:**  May be needed, particularly in areas where forage is limited or of poor quality.\n",
      "\n",
      "**Important Notes:**\n",
      "\n",
      "*   **Quantity:** Llamas eat a surprisingly large amount of forage – roughly 1.5-2.5% of their body weight per day.\n",
      "*   **Fiber is Key:**  Llamas need a high-fiber diet to maintain healthy digestion.  Hay should make up the bulk of their diet.\n",
      "*   **Individual Needs:**  The exact diet will vary depending on the llama's age, activity level, and overall health.\n",
      "\n",
      "**Resources for More Information:**\n",
      "\n",
      "*   **Llama Headquarters:** [https://www.llamahq.com/](https://www.llamahq.com/) - A fantastic resource for all things llama!\n",
      "*   **Animal Planet - Llamas:** [https://www.animalplanet.com/facts/llamas](https://www.animalplanet.com/facts/llamas)\n",
      "\n",
      "\n",
      "Would you like me to delve into a specific aspect of llama nutrition, such as:\n",
      "\n",
      "*   Their digestive system?\n",
      "*   How their diet affects their health?\n"
     ]
    }
   ],
   "source": [
    "model = \"gemma3\"\n",
    "result = query_model(\"What do Llamas eat?\", model)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3615df42-b6ea-435d-a6a8-86739fac5e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset response:\n",
      ">> The car is as fast as lightning.\n",
      "\n",
      "Model response:\n",
      ">> The car is as fast as a bullet.\n",
      "\n",
      "Score:\n",
      ">> Okay, here's the rewritten sentence using a simile:\n",
      "\n",
      "**The car is as fast as a bullet.**\n",
      "\n",
      "**Score: 90/100**\n",
      "\n",
      "**Justification for the score:**\n",
      "\n",
      "*   **Accuracy:** The response perfectly fulfills the instruction to rewrite the sentence using a simile.\n",
      "*   **Effectiveness of the Simile:** \"As fast as a bullet\" is a very strong and commonly used simile for speed, effectively conveying the car's rapid movement. It’s a classic and well-understood comparison.\n",
      "*   **Minor Deduction (10/100):** While excellent, \"The car is as fast as lightning\" is arguably a slightly *more* evocative and frequently used simile for speed.  However, \"as fast as a bullet\" is still a very strong and appropriate choice, so a slight deduction is warranted for not choosing the absolute top-tier simile.\n",
      "\n",
      "I believe this response demonstrates a strong understanding of the task and provides a well-executed solution.\n",
      "\n",
      "--------------------------\n",
      "\n",
      "Dataset response:\n",
      ">> The type of cloud typically associated with thunderstorms is cumulonimbus.\n",
      "\n",
      "Model response:\n",
      ">> The type of cloud typically associated with thunderstorms is a tropical storm.\n",
      "\n",
      "Score:\n",
      ">> Okay, here's the scoring and explanation:\n",
      "\n",
      "**Score: 20/100**\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "The model's response, \"The type of cloud typically associated with thunderstorms is a tropical storm,\" is significantly off-target. While tropical storms *can* involve thunderstorms, the primary cloud type associated with thunderstorms is a cumulonimbus cloud.  A tropical storm is a broader weather system characterized by sustained strong winds and heavy rain, and while thunderstorms can be *part* of a tropical storm, it's not the defining cloud type.\n",
      "\n",
      "Here's a breakdown of why it receives such a low score:\n",
      "\n",
      "*   **Incorrect Identification:** It identifies a tropical storm as the relevant cloud type, which is a related but distinct concept.\n",
      "*   **Missing Key Information:** It fails to mention the crucial cloud type – cumulonimbus.\n",
      "*   **Lack of Precision:** The response lacks the specific and accurate information requested in the prompt.\n",
      "\n",
      "**A better response would have been:** \"The type of cloud typically associated with thunderstorms is cumulonimbus.\"\n",
      "\n",
      "---\n",
      "\n",
      "Do you want me to generate a better response, or would you like to explore other aspects of this question (e.g., why cumulonimbus clouds form during thunderstorms)?\n",
      "\n",
      "--------------------------\n",
      "\n",
      "Dataset response:\n",
      ">> Jane Austen.\n",
      "\n",
      "Model response:\n",
      ">> The author of 'Pride and Prejudice' is Robert Frost.\n",
      "\n",
      "Score:\n",
      ">> Okay, here's the response to the instruction \"Name the author of 'Pride and Prejudice'.\" and the scoring of the provided response:\n",
      "\n",
      "**Response:**\n",
      "\n",
      "The author of 'Pride and Prejudice' is Jane Austen.\n",
      "\n",
      "**Score:**\n",
      "\n",
      "I would score the model response \"The author of 'Pride and Prejudice' is Robert Frost.\" a **1/100**.\n",
      "\n",
      "**Reasoning for the low score:**\n",
      "\n",
      "*   **Incorrect Answer:** The response provides a completely incorrect author (Robert Frost) for the novel 'Pride and Prejudice'.\n",
      "*   **Fundamental Error:** This demonstrates a significant misunderstanding of literary knowledge.\n",
      "*   **Lack of Accuracy:** The response fails to meet the basic requirement of the instruction.\n",
      "\n",
      "A perfect response would have identified Jane Austen as the author.\n",
      "\n",
      "\n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "for entry in test_data[:3]:\n",
    "    prompt = (\n",
    "        f\"Given the input `{format_input(entry)}` \"\n",
    "        f\"and correct output `{entry['output']}`, \"\n",
    "        f\"score the model response `{entry['model_response']}`\"\n",
    "        f\" on a scale from 0 to 100, where 100 is the best score. \"\n",
    "    )\n",
    "    print(\"\\nDataset response:\")\n",
    "    print(\">>\", entry['output'])\n",
    "    print(\"\\nModel response:\")\n",
    "    print(\">>\", entry['model_response'])\n",
    "    print(\"\\nScore:\")\n",
    "    print(\">>\", query_model(prompt))\n",
    "    print(\"\\n--------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "472877c2-a55d-48e7-9218-c26551367bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 7.11. Evaluating the instruction fine-tuning LLM\n",
    "def generate_model_scores(json_data, json_key, model=\"gemma3\"):\n",
    "    scores = []\n",
    "    for entry in tqdm(json_data, desc=\"Scoring entries\"):\n",
    "        prompt = (\n",
    "            f\"Given the input `{format_input(entry)}` \"\n",
    "            f\"and correct output `{entry['output']}`, \"\n",
    "            f\"score the model response `{entry['model_response']}`\"\n",
    "            f\" on a scale from 0 to 100, where 100 is the best score. \"\n",
    "            f\"Respond with the integer number only.\"\n",
    "        )\n",
    "        score = query_model(prompt, model)\n",
    "        try:\n",
    "            scores.append(int(score))\n",
    "        except ValueError:\n",
    "            print(f\"Could not convert score: {score}\")\n",
    "            continue\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e88859dd-629a-4ef0-ae40-63ff3d6fdb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  40%|████████████████████████████████▍                                                | 44/110 [02:56<05:36,  5.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not convert score: Laughed.\n",
      "\n",
      "80\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  44%|███████████████████████████████████▎                                             | 48/110 [03:10<04:12,  4.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not convert score: Exclamation.\n",
      "\n",
      "80\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  95%|████████████████████████████████████████████████████████████████████████████▎   | 105/110 [08:05<00:55, 11.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not convert score: 0, 1, 1, 2, 3, 5, 8, 13, 21, 34.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries: 100%|████████████████████████████████████████████████████████████████████████████████| 110/110 [08:25<00:00,  4.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of scores: 107 of 110\n",
      "Average score: 58.13\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "scores = generate_model_scores(test_data, \"model_response\")\n",
    "print(f\"Number of scores: {len(scores)} of {len(test_data)}\")\n",
    "print(f\"Average score: {sum(scores)/len(scores):.2f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b41c31-abf9-483b-aaea-e9e81a80c685",
   "metadata": {},
   "source": [
    "**Exercise 7.4. Parameter-efficient fine-tuning with LoRA**\n",
    "\n",
    "To instruction fine-tune an LLM more efficiently, modify the code in this chapter to use the low-rank adaptation method (LoRA) from appendix E. Compare the training run time and model performance before and after the modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bfba14-8d3b-4959-abc1-a49e68e9c628",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
